{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb5d487-55de-4661-9d74-342b83c61e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_rl import *\n",
    "import argparse, os\n",
    "import envs\n",
    "from gym.spaces.discrete import Discrete\n",
    "from gym.spaces.box import Box\n",
    "#from IPython import embed\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb2c549-c355-41e5-9166-281cac51501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_steps_improved(agent):\n",
    "    print('Start Running Steps!')\n",
    "    # print(f'Algorithm: {config.alg}')\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    agent.epoch = 0\n",
    "\n",
    "    test_performance = []\n",
    "    best_exploit_performance = -math.inf\n",
    "    best_exploit_params = None\n",
    "    logtxt(agent.logger.log_dir + '.csv', 'episodes, mean episode reward', date=False)\n",
    " \n",
    "    while True:\n",
    "        total_episodes = agent.total_steps / config.horizon\n",
    "        if config.save_interval and not agent.total_steps % config.save_interval:\n",
    "            print(\"save\")\n",
    "            agent.save('data/%s-%s-%d' % (agent_name, config.tag, agent.total_steps))\n",
    "        if agent.total_steps > 0 and config.log_interval and not total_episodes % config.log_interval and ('ppo-rpg' not in config.alg) and len(agent.ep_rewards) > 0:\n",
    "            running_mean_reward_10_ep = np.mean(agent.ep_rewards[-10:])\n",
    "            running_mean_reward_100_ep = np.mean(agent.ep_rewards[-100:])\n",
    "            running_mean_reward_1000_ep = np.mean(agent.ep_rewards[-1000:])\n",
    "            log_string = 'steps %d, episodes %d, %.2f steps/s, total rew %.2f, mean rew (10 ep) %.4f, mean rew (1000 ep) %.4f' % (agent.total_steps, total_episodes, config.log_interval / (time.time() - t0), agent.cumulative_reward, running_mean_reward_10_ep, running_mean_reward_1000_ep)\n",
    "            agent.logger.info(log_string)\n",
    "            logtxt(agent.logger.log_dir + '.txt', log_string)\n",
    "            t0 = time.time()\n",
    "            logtxt(agent.logger.log_dir + '.csv', f'{total_episodes},{running_mean_reward_100_ep}, {running_mean_reward_1000_ep}', date=False)\n",
    "            \n",
    "        if config.eval_interval and not agent.total_steps % config.eval_interval:\n",
    "            agent.eval_episodes()\n",
    "        if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "            agent.close()\n",
    "            break\n",
    "        if 'ppo-rpg' in config.alg:\n",
    "            agent.log(f'\\n###### EPOCH {agent.epoch} #####')\n",
    "            if config.alg == 'ppo-rpg':\n",
    "                if agent.epoch == agent.config.start_exploit:\n",
    "                    agent.initialize_new_policy('exploit')\n",
    "                \n",
    "                print('Start Evaluating')\n",
    "                avg_episodic_return = agent.eval_episodes()['episodic_return_test']\n",
    "                test_performance.append(avg_episodic_return)\n",
    "                #if avg_episodic_return > best_exploit_performance:\n",
    "                #    best_exploit_performance = avg_episodic_return\n",
    "                #    best_exploit_params = copy.deepcopy(agent.network['exploit'].state_dict())\n",
    "\n",
    "                print(\"#### at epoch {}, avg episodic return is {}\".format(agent.epoch, avg_episodic_return))\n",
    "                print(test_performance)\n",
    "                \n",
    "                # set flag to update policy mixture set\n",
    "                if not (agent.epoch+1) % agent.config.retrain_interval:\n",
    "                    agent.config.flag = 1\n",
    "                \n",
    "                # update replay_buffer and update bonus\n",
    "                if not agent.epoch % agent.config.retrain_interval:\n",
    "                    agent.update_replay_buffer()\n",
    "                    agent.query_counter = 0\n",
    "                    agent.query_pool = []\n",
    "                    if agent.config.save:\n",
    "                        agent.plot_visitation('explore') # print visitation of policy-cover\n",
    "                        agent.plot_visitation('exploit') # print visitation of exploitation \n",
    "                    if agent.config.bonus != 'width':\n",
    "                        agent.update_density_model(mode='explore')\n",
    "                   \n",
    "                    if agent.config.save:          \n",
    "                        with open(str(agent.config.seed)+'_explore_visitation_'+str(agent.epoch)+'_'+agent.config.bonus, 'w') as f_explore: \n",
    "                            replay_buffer = agent.replay_buffer['explore']\n",
    "                            states = torch.cat(sum(replay_buffer, [])).tolist()\n",
    "                            write = csv.writer(f_explore) \n",
    "                            write.writerows(states) \n",
    "                        with open(str(agent.config.seed)+'_exploit_visitation_'+str(agent.epoch)+'_'+agent.config.bonus, 'w') as f_exploit: \n",
    "                            replay_buffer = agent.replay_buffer['exploit']\n",
    "                            states = torch.cat(sum(replay_buffer, [])).tolist()\n",
    "                            write = csv.writer(f_exploit) \n",
    "                            write.writerows(states) \n",
    "                    \n",
    "#                agent.update_density_model(mode='exploit') # useful to print exploit policy visitation\n",
    "            \n",
    "            elif config.alg == 'ppo-rpg2':\n",
    "                agent.update_replay_buffer()\n",
    "                agent.update_density_model(mode='explore-exploit')\n",
    "            \n",
    "            agent.optimize_policy()\n",
    "            agent.config.flag = 0\n",
    "            agent.epoch += 1\n",
    "\n",
    "            if agent.epoch == agent.config.max_epochs:\n",
    "                #logtxt(agent.logger.log_dir + '.txt', f'reverting to best policy with performance {best_exploit_performance:.4f}', show=True)\n",
    "                #agent.network['exploit'].load_state_dict(best_exploit_params)\n",
    "                #total_episodes = agent.total_steps / config.horizon\n",
    "                #n_eval_episodes = config.num_workers*agent.config.n_rollouts_for_density_est\n",
    "                #running_mean_reward = agent.eval_episodes(n_episodes = n_eval_episodes)['episodic_return_test']\n",
    "                #logtxt(agent.logger.log_dir + '.txt', f'final performance: {running_mean_reward}', date=False)            \n",
    "                #agent.close()\n",
    "                \n",
    "                #logtxt(agent.logger.log_dir + '.txt', f'{total_episodes + n_eval_episodes},{running_mean_reward}', date=False, show=True)            \n",
    "                #logtxt(agent.logger.log_dir + '.csv', f'{total_episodes + n_eval_episodes},{running_mean_reward}', date=False)\n",
    "                #torch.save(agent.replay_buffer, agent.logger.log_dir + '.buffer')\n",
    "                break\n",
    "            else:\n",
    "                logtxt(agent.logger.log_dir + '.txt', f'{total_episodes},{avg_episodic_return}', date=False)            \n",
    "                logtxt(agent.logger.log_dir + '.csv', f'{total_episodes},{avg_episodic_return}', date=False)            \n",
    "        else:\n",
    "            agent.step()\n",
    "        \n",
    "        agent.switch_task()\n",
    "\n",
    "    return test_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "740d6ff5-62de-4493-b51a-99f85ef8032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpg_feature(**kwargs):\n",
    "    generate_tag(kwargs)\n",
    "    kwargs.setdefault('log_level', 0)\n",
    "    config = Config()\n",
    "    config.merge(kwargs)\n",
    "    \n",
    "    # running setting\n",
    "    config.num_workers = 10\n",
    "    config.log_interval = 100\n",
    "    config.norm_rew_b = 0\n",
    "    config.norm_rew = 0\n",
    "    config.init_new_policy = 0\n",
    "    config.n_policy_loops = 10\n",
    "    config.n_traj_per_loop = 50\n",
    "    \n",
    "    if config.norm_rew_b == 1:\n",
    "        config.reward_bonus_normalizer = MeanStdNormalizer()\n",
    "    else:\n",
    "        config.reward_bonus_normalizer = RescaleNormalizer()\n",
    "\n",
    "    if config.norm_rew == 1:\n",
    "        config.reward_normalizer = MeanStdNormalizer()\n",
    "    \n",
    "    if config.system == 'gcr':\n",
    "        config.log_dir = './log/'\n",
    "    elif config.system == 'philly':\n",
    "        config.log_dir = os.getenv('PT_OUTPUT_DIR') + '/'\n",
    "\n",
    "    \n",
    "    config.task_fn = lambda: Task(config.game, num_envs=config.num_workers, single_process=True, seed=config.seed, horizon = config.horizon, noise=config.noise)\n",
    "    config.eval_env = Task(config.game, seed=config.seed, horizon=config.horizon, noise=config.noise)\n",
    "    # set action_space seed\n",
    "    config.eval_env.action_space.seed(config.seed)\n",
    "    \n",
    "    config.optimizer_fn = lambda params: torch.optim.RMSprop(params, config.lr, weight_decay = config.weight_decay)\n",
    "    lr2 = config.lr if config.lr2 == -1.0 else config.lr2\n",
    "    config.optimizer_fn2 = lambda params: torch.optim.RMSprop(params, lr2, weight_decay = config.weight_decay)\n",
    "    config.eval_episodes = 100 #eval over 100 trajectories\n",
    "\n",
    "    # construct observations from states\n",
    "    # 1. product; 2. inverse; 3. noise\n",
    "    config.noise_dim = 0\n",
    "    config.obs_type = 0\n",
    "    if config.obs_type == 2:\n",
    "        config.noise_dim = 5\n",
    "    config.noise_low = -0.1\n",
    "    config.noise_high = 0.1\n",
    "    config.obs_dim = config.state_dim + config.noise_dim\n",
    "    \n",
    "    if isinstance(config.task_fn().action_space, Box):\n",
    "        #config.task_fn().action_space._np_seed \n",
    "        config.network_fn = lambda: GaussianActorCriticNet(config.obs_dim, config.action_dim, FCBody(config.obs_dim))\n",
    "    elif isinstance(config.task_fn().action_space, Discrete):\n",
    "        config.network_fn = lambda: CategoricalActorCriticNet(config.obs_dim, config.action_dim, FCBody(config.obs_dim))\n",
    "    \n",
    "    \n",
    "    config.flag = 0\n",
    "    config.print = 0\n",
    "    config.save = 0 # set to 1 if plot visitations\n",
    "    config.counter = 0\n",
    "    config.plot = 0 # set to 1 if plot bonus functions\n",
    "        \n",
    "    # hyperparameters for width on Mountaincar\n",
    "    if config.game == 'MountainCarContinuous-v0' and config.bonus == 'width':\n",
    "        config.bonus_coeff = 0.005 # set 0 for test 'ZERO'\n",
    "        config.width_max = 0\n",
    "        config.width_gd_steps = 10\n",
    "        config.width_batch_size = 32 * 5\n",
    "        config.width_loss_lambda = 0.1\n",
    "        config.width_loss_lambda1 = 0.01\n",
    "        config.width_lr = 0.001 # =0.0015 for layer=6\n",
    "        config.width_gradient_clip = 5\n",
    "        config.retrain_interval = 3 # No. of NPG steps per epoch is retrain_interval * n_policy_loops\n",
    "        config.width_loop = 1000\n",
    "        config.query_size = 200000 # used to be 20000\n",
    "        config.query_batch = 20 # =10 for layer=6\n",
    "        config.copy = 0 # 0: initialize width_network once; 1: reinit every width_train; 2: copy 'explore' network\n",
    "        config.online = 0\n",
    "        config.beta = -1\n",
    "    \n",
    "    # hyperparameters for PCPG on Mountaincar\n",
    "    if config.game == 'MountainCarContinuous-v0' and config.bonus == 'rbf-kernel':\n",
    "        config.bonus_coeff = 0.01\n",
    "        config.retrain_interval = 3\n",
    "        config.beta = -1\n",
    "        \n",
    "    # hyperparameters for PPO\n",
    "    config.discount = 0.99\n",
    "    config.use_gae = True\n",
    "    config.gae_tau = 0.95\n",
    "    config.entropy_weight = 0.01\n",
    "    config.gradient_clip = 5\n",
    "    config.rollout_length = config.horizon\n",
    "    config.mini_batch_size = 32 * 5\n",
    "    config.optimization_epochs = 5\n",
    "    config.ppo_ratio_clip = 0.2\n",
    "    config.max_steps = 10e9*config.horizon\n",
    "    config.start_exploit = config.horizon if 'combolock' in config.game else 0\n",
    "    if 'combolock' in config.game:\n",
    "        config.start_exploit = config.horizon \n",
    "        config.max_epochs = 3*config.horizon\n",
    "        config.rmax = 5.0\n",
    "        config.n_rollouts_for_density_est = 50\n",
    "    else:\n",
    "        config.start_exploit = 3\n",
    "        config.max_epochs = 10\n",
    "        if config.game == 'MountainCarContinuous-v0':\n",
    "            config.rmax = 100 # hardcoding for now\n",
    "        config.n_rollouts_for_density_est = 10\n",
    "        #config.state_normalizer = TwoRescaleNormalizer(1., 10.)\n",
    "    \n",
    "    print('Start Running!')\n",
    "    config.ridge = 0.01\n",
    "    if config.alg == 'ppo-rpg':\n",
    "        print('We are using ENIAC (RPGAgent)')\n",
    "        return run_steps_improved(RPGAgent(config))\n",
    "    elif config.alg == 'ppo-rpg2':\n",
    "        return run_steps_improved(RPG2Agent(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca446bde-923f-4410-b121-777c71a30948",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Running!\n",
      "We are using ENIAC (RPGAgent)\n",
      "Start Initializing Agent!\n",
      "Agent Created!\n",
      "Start Running Steps!\n",
      "\n",
      "###### EPOCH 0 #####\n",
      "Start Evaluating\n",
      "#### at epoch 0, avg episodic return is -36.14052018863532\n",
      "[-36.14052018863532]\n",
      "[gathering trajectories for replay buffer]\n",
      "[policy mixture returns: [-3.615]]\n",
      "tensor([1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.00000]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.02115]\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "\n",
      "###### EPOCH 1 #####\n",
      "Start Evaluating\n",
      "#### at epoch 1, avg episodic return is -36.276912488905225\n",
      "[-36.14052018863532, -36.276912488905225]\n",
      "tensor([1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.12078]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.40283]\n",
      "\n",
      "###### EPOCH 2 #####\n",
      "Start Evaluating\n",
      "#### at epoch 2, avg episodic return is -36.44304744478649\n",
      "[-36.14052018863532, -36.276912488905225, -36.44304744478649]\n",
      "tensor([1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.70766]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.97157]\n",
      "2 policies in mixture\n",
      "\n",
      "###### EPOCH 3 #####\n",
      "Start Evaluating\n",
      "#### at epoch 3, avg episodic return is -35.996781860407125\n",
      "[-36.14052018863532, -36.276912488905225, -36.44304744478649, -35.996781860407125]\n",
      "[gathering trajectories for replay buffer]\n",
      "[policy mixture returns: [ -3.615 127.343]]\n",
      "tensor([1., 1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 1.15716]\n",
      "[optimizing policy (explore), step 5, mean reward: 1.29097]\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[optimizing policy (exploit), step 0, mean reward: 0.06292]\n",
      "[optimizing policy (exploit), step 5, mean reward: 0.12570]\n",
      "\n",
      "###### EPOCH 4 #####\n",
      "Start Evaluating\n",
      "#### at epoch 4, avg episodic return is 87.19499465872559\n",
      "[-36.14052018863532, -36.276912488905225, -36.44304744478649, -35.996781860407125, 87.19499465872559]\n",
      "tensor([1., 1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 1.69518]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-9eece71d6a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     delay=config.delay)\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mperformance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mperformance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_performance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_performance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9796cb153078>\u001b[0m in \u001b[0;36mrpg_feature\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ppo-rpg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'We are using ENIAC (RPGAgent)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_steps_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRPGAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ppo-rpg2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_steps_improved\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRPG2Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fd3f1b947f16>\u001b[0m in \u001b[0;36mrun_steps_improved\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_density_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'explore-exploit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workplace/width/ENIAC/code/DeepRL/deep_rl/agent/RPG_agent.py\u001b[0m in \u001b[0;36moptimize_policy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exploit'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_exploit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_policy_loops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m                 \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_optimize_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workplace/width/ENIAC/code/DeepRL/deep_rl/agent/RPG_agent.py\u001b[0m in \u001b[0;36mstep_optimize_policy\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m                 \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_bonus_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'explore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#from mixture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_bonus_reward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'explore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroll_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workplace/width/ENIAC/code/DeepRL/deep_rl/agent/RPG_agent.py\u001b[0m in \u001b[0;36mgather_trajectories\u001b[0;34m(self, roll_in, add_bonus_reward, debug, mode, record_return)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_normalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_normalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m             storage.add({'r': tensor(rewards).unsqueeze(-1),\n\u001b[1;32m    422\u001b[0m                          \u001b[0;34m'm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workplace/width/ENIAC/code/DeepRL/deep_rl/utils/torch_utils.py\u001b[0m in \u001b[0;36mtensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pcpg/lib/python3.6/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# without width\n",
    "if __name__ == '__main__':\n",
    "    mkdir('log')\n",
    "    mkdir('tf_log')\n",
    "    select_device(0)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-device', type=int, default=0)\n",
    "    parser.add_argument('-alg', type=str, default='ppo-rpg')\n",
    "    parser.add_argument('-env', type=str, default='MountainCarContinuous-v0')\n",
    "    parser.add_argument('-horizon', type=int, default=100)\n",
    "    parser.add_argument('-noise', type=str, default='bernoulli')\n",
    "    parser.add_argument('-eps', type=float, default=0.05)\n",
    "    parser.add_argument('-lr', type=float, default=0.0005)\n",
    "    parser.add_argument('-lr2', type=float, default=-1.0)\n",
    "    parser.add_argument('-seed', type=int, default=0)\n",
    "    parser.add_argument('-rnd_l2', type=float, default=0.0)\n",
    "    parser.add_argument('-proll', type=float, default=0.8)\n",
    "    parser.add_argument('-rnd_bneck', type=int, default=4)\n",
    "    parser.add_argument('-bonus_coeff', type=float, default=0.01)\n",
    "    parser.add_argument('-bonus_choice', type=int, default=1)  # 1 is max(bonus, rewards); 2 is rewards += bonus\n",
    "    parser.add_argument('-bonus_select', type=int, default=2)  # 1 is permutation; 2 is uniform sampling; 3 is sequential \n",
    "    parser.add_argument('-bonus', type=str, default='width')\n",
    "    parser.add_argument('-w_q', type=float, default = 0.85)\n",
    "    parser.add_argument('-layer', type=int, default=2)\n",
    "    parser.add_argument('-n_policy_loops', type=int, default=10)\n",
    "    parser.add_argument('-n_traj_per_loop', type=int, default=50)\n",
    "    parser.add_argument('-init_new_policy', type=int, default=0)\n",
    "    parser.add_argument('-norm_rew', type=int, default=0)\n",
    "    parser.add_argument('-norm_rew_b', type=int, default=0)\n",
    "    parser.add_argument('-phi_dim', type=int, default=64)\n",
    "    parser.add_argument('-beta', type=float, default=-1)\n",
    "    parser.add_argument('-weight_decay', type=float, default=0)\n",
    "    parser.add_argument('-obs_type', type=int, default=0)\n",
    "    parser.add_argument('-retrain_interval', type=int, default=3)\n",
    "    parser.add_argument('-system', type=str, default='gcr')\n",
    "    parser.add_argument('-save', type=int, default=0)\n",
    "    \n",
    "    # the following are parameters for OSPOE\n",
    "    parser.add_argument('-delay', type=int, default=1) # delay == 1 -> ENIAC\n",
    "    \n",
    "    config = parser.parse_args([])\n",
    "    select_device(config.device)\n",
    "    random_seed(config.seed)\n",
    "    \n",
    "    performance = None\n",
    "    filename = f'{config.alg}-delay{config.delay}'\n",
    "    \n",
    "    if config.alg == 'ppo':\n",
    "        if config.env == 'MontezumaRevengeNoFrameskip-v4':\n",
    "            performance = ppo_pixel(game=config.env,\n",
    "                        lr=config.lr,\n",
    "                        seed=config.seed,\n",
    "                        rnd = 0,\n",
    "                        alg='ppo',\n",
    "                        system = config.system)\n",
    "        else:\n",
    "            performance = ppo_feature(game=config.env,\n",
    "                        lr=config.lr,\n",
    "                        horizon=config.horizon,\n",
    "                        noise = config.noise, \n",
    "                        seed=config.seed,\n",
    "                        eps = config.eps,\n",
    "                        rnd = 0,\n",
    "                        alg='ppo',\n",
    "                        system = config.system)        \n",
    "    elif config.alg == 'ppo-rnd':\n",
    "        performance = ppo_feature(game=config.env,\n",
    "                    lr=config.lr,\n",
    "                    horizon=config.horizon,\n",
    "                    noise = config.noise,\n",
    "                    seed=config.seed,\n",
    "                    rnd = 1,\n",
    "                    rnd_l2 = config.rnd_l2,\n",
    "                    rnd_bneck = config.rnd_bneck,\n",
    "                    eps = config.eps,\n",
    "                    phi_dim = config.phi_dim,\n",
    "                    rnd_bonus = config.bonus_coeff,\n",
    "                    alg='ppo-rnd',\n",
    "                    norm_rew=config.norm_rew,\n",
    "                    norm_rew_b=config.norm_rew_b,\n",
    "                    system = config.system)\n",
    "    elif config.alg in ['ppo-rpg', 'ppo-rpg2']:\n",
    "        for i in range(2):\n",
    "            cur_performance = rpg_feature(game=config.env,\n",
    "                    lr=config.lr,\n",
    "                    lr2=config.lr2,\n",
    "                    horizon=config.horizon,\n",
    "                    noise = config.noise, \n",
    "                    seed=i,\n",
    "                    eps = config.eps,\n",
    "                    proll = config.proll,\n",
    "                    bonus = config.bonus,\n",
    "                    bonus_coeff = config.bonus_coeff,\n",
    "                    bonus_choice = config.bonus_choice,\n",
    "                    bonus_select = config.bonus_select,\n",
    "                    beta = config.beta,\n",
    "                    w_q = config.w_q,\n",
    "                    phi_dim = config.phi_dim, \n",
    "                    alg=config.alg,\n",
    "                    system = config.system,\n",
    "                    layer = config.layer,\n",
    "                    weight_decay = config.weight_decay,\n",
    "                    delay=config.delay)\n",
    "            if performance is None:\n",
    "                performance = np.array(cur_performance).reshape(len(cur_performance), 1)\n",
    "            else:\n",
    "                cur_performance = np.array(cur_performance).reshape(len(cur_performance), 1)\n",
    "                performance = np.concatenate([performance, cur_performance], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba86bce5-cc1c-49f7-8a7f-bf2fd2874815",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('OSPOE-performance.npy', performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3f2d65-51bb-4949-a4af-61ee1424cf7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Running!\n",
      "We are using ENIAC (RPGAgent)\n",
      "Start Initializing Agent!\n",
      "Agent Created!\n",
      "Start Running Steps!\n",
      "\n",
      "###### EPOCH 0 #####\n",
      "Start Evaluating\n",
      "#### at epoch 0, avg episodic return is -36.14052018863532\n",
      "[-36.14052018863532]\n",
      "[gathering trajectories for replay buffer]\n",
      "[policy mixture returns: [-3.615]]\n",
      "tensor([1.])\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[optimizing policy (explore), step 0, mean reward: 0.08695]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.10535]\n",
      "\n",
      "###### EPOCH 1 #####\n",
      "Start Evaluating\n",
      "#### at epoch 1, avg episodic return is -36.30206056220105\n",
      "[-36.14052018863532, -36.30206056220105]\n",
      "tensor([1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.11047]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.11071]\n",
      "\n",
      "###### EPOCH 2 #####\n",
      "Start Evaluating\n",
      "#### at epoch 2, avg episodic return is -36.4490973357159\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159]\n",
      "tensor([1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.11136]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.11227]\n",
      "2 policies in mixture\n",
      "\n",
      "###### EPOCH 3 #####\n",
      "Start Evaluating\n",
      "#### at epoch 3, avg episodic return is -36.003259826436576\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576]\n",
      "[gathering trajectories for replay buffer]\n",
      "[policy mixture returns: [-3.615 -4.674]]\n",
      "tensor([1., 1.])\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[optimizing policy (explore), step 0, mean reward: 0.10800]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.11761]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.03627]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.01259]\n",
      "\n",
      "###### EPOCH 4 #####\n",
      "Start Evaluating\n",
      "#### at epoch 4, avg episodic return is -4.282835930353064\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576, -4.282835930353064]\n",
      "tensor([1., 1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.12875]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.13411]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.00431]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.00199]\n",
      "\n",
      "###### EPOCH 5 #####\n",
      "Start Evaluating\n",
      "#### at epoch 5, avg episodic return is -0.8346087530498238\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576, -4.282835930353064, -0.8346087530498238]\n",
      "tensor([1., 1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.13495]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.13480]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.00087]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.00040]\n",
      "3 policies in mixture\n",
      "\n",
      "###### EPOCH 6 #####\n",
      "Start Evaluating\n",
      "#### at epoch 6, avg episodic return is -0.14456470591289222\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576, -4.282835930353064, -0.8346087530498238, -0.14456470591289222]\n",
      "[gathering trajectories for replay buffer]\n",
      "[policy mixture returns: [-3.615 -4.674 -4.332]]\n",
      "tensor([1., 1., 1.])\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[optimizing policy (explore), step 0, mean reward: 0.27533]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.31306]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.00016]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.00007]\n",
      "\n",
      "###### EPOCH 7 #####\n",
      "Start Evaluating\n",
      "#### at epoch 7, avg episodic return is -0.026334709207584784\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576, -4.282835930353064, -0.8346087530498238, -0.14456470591289222, -0.026334709207584784]\n",
      "tensor([1., 1., 1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.34589]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.34775]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.00003]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.00003]\n",
      "\n",
      "###### EPOCH 8 #####\n",
      "Start Evaluating\n",
      "#### at epoch 8, avg episodic return is -0.007269469012412297\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576, -4.282835930353064, -0.8346087530498238, -0.14456470591289222, -0.026334709207584784, -0.007269469012412297]\n",
      "tensor([1., 1., 1.])\n",
      "[optimizing policy (explore), step 0, mean reward: 0.35219]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.36568]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.00006]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.00006]\n",
      "4 policies in mixture\n",
      "\n",
      "###### EPOCH 9 #####\n",
      "Start Evaluating\n",
      "#### at epoch 9, avg episodic return is -0.006482610082221895\n",
      "[-36.14052018863532, -36.30206056220105, -36.4490973357159, -36.003259826436576, -4.282835930353064, -0.8346087530498238, -0.14456470591289222, -0.026334709207584784, -0.007269469012412297, -0.006482610082221895]\n",
      "[gathering trajectories for replay buffer]\n",
      "[policy mixture returns: [-3.615 -4.674 -4.332 -3.118]]\n",
      "tensor([1., 1., 1., 1.])\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[Retrain Width!]\n",
      "=========== Retraining width! ============\n",
      "[optimizing policy (explore), step 0, mean reward: 0.28084]\n",
      "[optimizing policy (explore), step 5, mean reward: 0.29043]\n",
      "[optimizing policy (exploit), step 0, mean reward: -0.00001]\n",
      "[optimizing policy (exploit), step 5, mean reward: -0.00005]\n"
     ]
    }
   ],
   "source": [
    "# with width\n",
    "if __name__ == '__main__':\n",
    "    mkdir('log')\n",
    "    mkdir('tf_log')\n",
    "    select_device(0)\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-device', type=int, default=0)\n",
    "    parser.add_argument('-alg', type=str, default='ppo-rpg')\n",
    "    parser.add_argument('-env', type=str, default='MountainCarContinuous-v0')\n",
    "    parser.add_argument('-horizon', type=int, default=100)\n",
    "    parser.add_argument('-noise', type=str, default='bernoulli')\n",
    "    parser.add_argument('-eps', type=float, default=0.05)\n",
    "    parser.add_argument('-lr', type=float, default=0.0005)\n",
    "    parser.add_argument('-lr2', type=float, default=-1.0)\n",
    "    parser.add_argument('-seed', type=int, default=0)\n",
    "    parser.add_argument('-rnd_l2', type=float, default=0.0)\n",
    "    parser.add_argument('-proll', type=float, default=0.8)\n",
    "    parser.add_argument('-rnd_bneck', type=int, default=4)\n",
    "    parser.add_argument('-bonus_coeff', type=float, default=0.01)\n",
    "    parser.add_argument('-bonus_choice', type=int, default=1)  # 1 is max(bonus, rewards); 2 is rewards += bonus\n",
    "    parser.add_argument('-bonus_select', type=int, default=2)  # 1 is permutation; 2 is uniform sampling; 3 is sequential \n",
    "    parser.add_argument('-bonus', type=str, default='width')\n",
    "    parser.add_argument('-w_q', type=float, default = 0.85)\n",
    "    parser.add_argument('-layer', type=int, default=2)\n",
    "    parser.add_argument('-n_policy_loops', type=int, default=10)\n",
    "    parser.add_argument('-n_traj_per_loop', type=int, default=50)\n",
    "    parser.add_argument('-init_new_policy', type=int, default=0)\n",
    "    parser.add_argument('-norm_rew', type=int, default=0)\n",
    "    parser.add_argument('-norm_rew_b', type=int, default=0)\n",
    "    parser.add_argument('-phi_dim', type=int, default=64)\n",
    "    parser.add_argument('-beta', type=float, default=-1)\n",
    "    parser.add_argument('-weight_decay', type=float, default=0)\n",
    "    parser.add_argument('-obs_type', type=int, default=0)\n",
    "    parser.add_argument('-retrain_interval', type=int, default=3)\n",
    "    parser.add_argument('-system', type=str, default='gcr')\n",
    "    parser.add_argument('-save', type=int, default=0)\n",
    "    \n",
    "    # the following are parameters for OSPOE\n",
    "    parser.add_argument('-delay', type=int, default=1) # delay == 1 -> ENIAC\n",
    "    \n",
    "    config = parser.parse_args([])\n",
    "    select_device(config.device)\n",
    "    random_seed(config.seed)\n",
    "    \n",
    "    performance = None\n",
    "    filename = f'{config.alg}-delay{config.delay}'\n",
    "    \n",
    "    if config.alg == 'ppo':\n",
    "        if config.env == 'MontezumaRevengeNoFrameskip-v4':\n",
    "            performance = ppo_pixel(game=config.env,\n",
    "                        lr=config.lr,\n",
    "                        seed=config.seed,\n",
    "                        rnd = 0,\n",
    "                        alg='ppo',\n",
    "                        system = config.system)\n",
    "        else:\n",
    "            performance = ppo_feature(game=config.env,\n",
    "                        lr=config.lr,\n",
    "                        horizon=config.horizon,\n",
    "                        noise = config.noise, \n",
    "                        seed=config.seed,\n",
    "                        eps = config.eps,\n",
    "                        rnd = 0,\n",
    "                        alg='ppo',\n",
    "                        system = config.system)        \n",
    "    elif config.alg == 'ppo-rnd':\n",
    "        performance = ppo_feature(game=config.env,\n",
    "                    lr=config.lr,\n",
    "                    horizon=config.horizon,\n",
    "                    noise = config.noise,\n",
    "                    seed=config.seed,\n",
    "                    rnd = 1,\n",
    "                    rnd_l2 = config.rnd_l2,\n",
    "                    rnd_bneck = config.rnd_bneck,\n",
    "                    eps = config.eps,\n",
    "                    phi_dim = config.phi_dim,\n",
    "                    rnd_bonus = config.bonus_coeff,\n",
    "                    alg='ppo-rnd',\n",
    "                    norm_rew=config.norm_rew,\n",
    "                    norm_rew_b=config.norm_rew_b,\n",
    "                    system = config.system)\n",
    "    elif config.alg in ['ppo-rpg', 'ppo-rpg2']:\n",
    "        for i in range(1):\n",
    "            cur_performance = rpg_feature(game=config.env,\n",
    "                    lr=config.lr,\n",
    "                    lr2=config.lr2,\n",
    "                    horizon=config.horizon,\n",
    "                    noise = config.noise, \n",
    "                    seed=i,\n",
    "                    eps = config.eps,\n",
    "                    proll = config.proll,\n",
    "                    bonus = config.bonus, \n",
    "                    bonus_coeff = config.bonus_coeff,\n",
    "                    bonus_choice = config.bonus_choice,\n",
    "                    bonus_select = config.bonus_select,\n",
    "                    beta = config.beta,\n",
    "                    w_q = config.w_q,\n",
    "                    phi_dim = config.phi_dim, \n",
    "                    alg=config.alg,\n",
    "                    system = config.system,\n",
    "                    layer = config.layer,\n",
    "                    weight_decay = config.weight_decay,\n",
    "                    delay=config.delay)\n",
    "            if performance is None:\n",
    "                performance = np.array(cur_performance).reshape(len(cur_performance), 1)\n",
    "            else:\n",
    "                cur_performance = np.array(cur_performance).reshape(len(cur_performance), 1)\n",
    "                performance = np.concatenate([performance, cur_performance], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc14bc-5687-4eec-b6bb-64d384f93991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcpg",
   "language": "python",
   "name": "pcpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
